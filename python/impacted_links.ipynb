{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filepaths for the all_links delay, incident_link delay, and feeder_links list\n",
    "def safe_float(value):\n",
    "    try:\n",
    "        return float(value) if value is not None and value != \"\" else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "all_links = '../data/link_delays'\n",
    "incident_link_delays = \"../data/incident_analysis/delay/incident_link_delays\"\n",
    "feeder_links = \"../data/feeder_links/feeders.csv\"\n",
    "NCE_duration = \"../data/change_events/NCE_Durations.csv\"\n",
    "\n",
    "if not os.path.exists(incident_link_delays):\n",
    "    os.makedirs(incident_link_delays)\n",
    "\n",
    "feeder_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feeder_links, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        seed = row['Seed']\n",
    "        links = {\n",
    "            row['Incident Link'],\n",
    "            row['Feeder 1'],\n",
    "            row['Feeder 2']\n",
    "        }\n",
    "\n",
    "        if seed not in feeder_data:\n",
    "            feeder_data[seed] = set()\n",
    "\n",
    "        for link_id in links:\n",
    "            if link_id:  # Ensure link_id is not empty\n",
    "                feeder_data[seed].add(link_id)\n",
    "\n",
    "baseline_path = os.path.join(all_links, \"Seed 000\", \"0-0-000.delay.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE TO WRITE INCIDENT LINK DELAY CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed, relevant_links in feeder_data.items():\n",
    "    seed_directory = f\"Seed {seed}\"\n",
    "    source_path = os.path.join(all_links, seed_directory)\n",
    "    target_path = os.path.join(incident_link_delays, seed_directory)\n",
    "\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    # Copy and filter baseline data\n",
    "    with open(baseline_path, 'r') as baseline_file, open(os.path.join(target_path, \"0-0-000.delay.csv\"), 'w', newline='') as f_out:\n",
    "        reader = csv.DictReader(baseline_file)\n",
    "        fieldnames = reader.fieldnames\n",
    "        if 'Total Delay [hours]' not in fieldnames:\n",
    "            fieldnames.append('Total Delay [hours]')\n",
    "        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            link_id = row['Link Id']\n",
    "            if link_id in relevant_links:\n",
    "                delay_columns = [col for col in reader.fieldnames if ':' in col]\n",
    "                for col in delay_columns:\n",
    "                    if not row.get(col):\n",
    "                        row[col] = \"0.0\"\n",
    "                total_delay = sum(safe_float(row[col]) for col in delay_columns) / 3600\n",
    "                row['Total Delay [hours]'] = total_delay\n",
    "                writer.writerow(row)\n",
    "\n",
    "    # Process and filter data for other delay files in the seed directory\n",
    "    for delay_file in os.listdir(source_path):\n",
    "        with open(os.path.join(source_path, delay_file), 'r') as f_in, open(os.path.join(target_path, delay_file), 'w', newline='') as f_out:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            fieldnames = reader.fieldnames\n",
    "            if 'Total Delay [hours]' not in fieldnames:\n",
    "                fieldnames.append('Total Delay [hours]')\n",
    "            writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                link_id = row['Link Id']\n",
    "                if link_id in relevant_links:\n",
    "                    delay_columns = [col for col in reader.fieldnames if ':' in col]\n",
    "                    for col in delay_columns:\n",
    "                        if not row.get(col):\n",
    "                            row[col] = \"0.0\"\n",
    "                    total_delay = sum(safe_float(row[col]) for col in delay_columns) / 3600\n",
    "                    row['Total Delay [hours]'] = total_delay\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD LABELS TO THE LINKS TO SPECIFY IF THEY ARE INCIDENT OR FEEDER LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characterizations(feeder_clean_path):\n",
    "    with open(feeder_clean_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        char_dict = {}\n",
    "        for row in reader:\n",
    "            seed = row['Seed']\n",
    "            entry = {\n",
    "                'Incident Link': row['Incident Link'],\n",
    "                'Feeder 1': row['Feeder 1'],\n",
    "                'Feeder 2': row['Feeder 2']\n",
    "            }\n",
    "            if seed not in char_dict:\n",
    "                char_dict[seed] = []\n",
    "            char_dict[seed].append(entry)\n",
    "        return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the time utility functions here\n",
    "def add_time(time_str, minutes_to_add):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    m += minutes_to_add\n",
    "    h += m // 60\n",
    "    m %= 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def round_down(time_str):\n",
    "    h, m, _ = map(int, time_str.split(':'))\n",
    "    m = (m // 15) * 15\n",
    "    return f\"{h:02d}:{m:02d}:00\"\n",
    "\n",
    "def time_to_minutes_since_start(time_str):\n",
    "    \"\"\"Converts HH:MM:SS format to minutes since start of day.\"\"\"\n",
    "    h, m, _ = map(int, time_str.split(':'))\n",
    "    return h * 60 + m\n",
    "\n",
    "def is_time_format(s):\n",
    "    \"\"\"Check if the string is in the HH:MM:SS format.\"\"\"\n",
    "    try:\n",
    "        h, m, s = map(int, s.split(':'))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_row_delay(row, start_period, end_period):\n",
    "    start_minutes = time_to_minutes_since_start(start_period)\n",
    "    end_minutes = time_to_minutes_since_start(end_period)\n",
    "    columns_to_sum = [col for col in row.index if is_time_format(col) and time_to_minutes_since_start(col) >= start_minutes and time_to_minutes_since_start(col) <= end_minutes]\n",
    "    return row[columns_to_sum].sum() / 3600\n",
    "\n",
    "def compute_corrected_row_delay(row, end_period, duration_minutes):\n",
    "    end_minutes = time_to_minutes_since_start(end_period)\n",
    "    columns_to_sum = [col for col in row.index if is_time_format(col) and time_to_minutes_since_start(col) >= end_minutes and time_to_minutes_since_start(col) < end_minutes + duration_minutes]\n",
    "    return row[columns_to_sum].sum() / 3600\n",
    "    \n",
    "def compute_post_incident_delay(row, end_period, duration_minutes):\n",
    "    end_minutes = time_to_minutes_since_start(end_period)\n",
    "    columns_to_sum = [col for col in row.index if is_time_format(col) and time_to_minutes_since_start(col) >= end_minutes and time_to_minutes_since_start(col) < end_minutes + duration_minutes]\n",
    "    return row[columns_to_sum].sum() / 3600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(incident_link_delays, char_dict):\n",
    "    for seed_dir in os.listdir(incident_link_delays):\n",
    "        seed = seed_dir.split(\" \")[-1]\n",
    "        path = os.path.join(incident_link_delays, seed_dir)\n",
    "        for delay_file in os.listdir(path):\n",
    "            delay_file_path = os.path.join(path, delay_file)\n",
    "            new_rows = []\n",
    "\n",
    "            with open(delay_file_path, 'r') as f_in:\n",
    "                reader = csv.DictReader(f_in)\n",
    "                for row in reader:\n",
    "                    link_id = row['Link Id']\n",
    "                    for entry in char_dict[seed]:\n",
    "                        for typ, associated_link in entry.items():\n",
    "                            if link_id == associated_link:\n",
    "                                new_row = row.copy()\n",
    "                                new_row['Type'] = typ\n",
    "                                new_row['Incident Link'] = entry['Incident Link']\n",
    "                                new_row['Feeder 1'] = entry['Feeder 1']\n",
    "                                new_row['Feeder 2'] = entry['Feeder 2']\n",
    "                                new_rows.append(new_row)\n",
    "                                break  # Break once we find a match\n",
    "\n",
    "            # write the new rows to the file\n",
    "            with open(delay_file_path, 'w', newline='') as f_out:\n",
    "                writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames + ['Type', 'Incident Link', 'Feeder 1', 'Feeder 2'])\n",
    "                writer.writeheader()\n",
    "                for row in new_rows:\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    char_dict = get_characterizations(feeder_links)\n",
    "    process_files(incident_link_delays, char_dict)\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Load NCE_Durations.csv\n",
    "nce_df = pd.read_csv(os.path.join(NCE_duration))\n",
    "\n",
    "for root, _, files in os.walk(incident_link_delays):\n",
    "    for file in files:\n",
    "        if file.endswith('.delay.csv'):\n",
    "            \n",
    "            # Extract the Seed value from the folder name\n",
    "            current_seed = int(os.path.basename(root).split(\" \")[1])  \n",
    "            \n",
    "            # Filter nce_df based on current_seed\n",
    "            filtered_nce_df = nce_df[nce_df['Seed'] == current_seed]\n",
    "            \n",
    "            delay_df = pd.read_csv(os.path.join(root, file))\n",
    "\n",
    "            # Merge with the filtered nce_df\n",
    "            matched_rows = delay_df.merge(filtered_nce_df, on=\"Incident Link\", how=\"left\")\n",
    "\n",
    "            matched_rows[\"Incident Start\"] = matched_rows[\"Start Time [HH:MM:SS]\"].apply(round_down)\n",
    "            matched_rows[\"Incident End\"] = matched_rows[\"End Time [HH:MM:SS]\"].apply(round_down)\n",
    "\n",
    "            matched_rows[\"Delay During Incident [hours]\"] = matched_rows.apply(lambda row: compute_row_delay(row, row[\"Incident Start\"], row[\"Incident End\"]), axis=1)\n",
    "            matched_rows[\"Post-Incident Delay (0-30 mins) [hours]\"] = matched_rows.apply(lambda row: compute_post_incident_delay(row, add_time(row[\"Incident End\"], 15), 30), axis=1)\n",
    "            matched_rows[\"Post-Incident Delay (30-60 mins) [hours]\"] = matched_rows.apply(lambda row: compute_post_incident_delay(row, add_time(row[\"Incident End\"], 45), 30), axis=1)\n",
    "\n",
    "            # Drop duplicate columns\n",
    "            matched_rows = matched_rows.T.drop_duplicates().T\n",
    "\n",
    "            matched_rows.drop(columns=[\"Start Time [HH:MM:SS]\", \"End Time [HH:MM:SS]\"], inplace=True, errors='ignore')\n",
    "            matched_rows.to_csv(os.path.join(root, file), index=False)\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
